# nGinx rate limiting config script by QWeb Ltd.
#
# Adds different HTTP rate limiting zones for known bots, services, and humans. Rate limiting prevents any one node from flooding your request queues, in turn stopping content scrapers and malicious parties from hogging resources and allowing your server to remain responsive to legitimate traffic.
#
# Simply add this file to your nginx config directory, for example /etc/nginx/plesk.conf.d/ip_default/nginx-rate-limiting.conf on a Plesk server, and edit the $known_ips block below.
# Actual rates for each zone are controlled by the limit_req_zone and limit_req lines at the end. 
#
# Don't forget to reload nginx after adding or editing this file! For example, on Systemd based systems, e.g. CentOS / Redhat / Rocky etc:
# sudo systemctl reload nginx
#
# By default, this limits regular traffic to 30 requests per second, authorised humans to 60 requests per second, Nitropack servers to 4 requests per second, and known bots to 5 or 10 requests per minute dependant on their usefulness.
#
# Refer to the nGinx config documentation for how all of this works:
#  https://nginx.org/en/docs/http/ngx_http_geo_module.html
#  https://nginx.org/en/docs/http/ngx_http_limit_req_module.html
#  https://nginx.org/en/docs/http/ngx_http_map_module.html
#
# Requires ngx_http_map_module, ngx_http_geo_module, and nGinx 1.0.4 or above: https://nginx.org/en/docs/http/ngx_http_map_module.html
#
#
#

# Useragents of known bots. Map to 1 to restrict them heavily, or 2 to still restrict more than humans but not too much
# General rule of thumb is to be lenient towards search engines and business directories etc, because they're potentially good for lead generation, and more restrictive to less useful services
# The purpose of this isn't to block, but to prevent resource drain. Individual websites wanting to block bots should do so via htaccess or similar
map $http_user_agent $known_bots {
	default 0;
	~*(008|AddSugarSpiderBot|AdsBot|AhrefsBot|AmazonBot|Bimbot|boitho\.com|Bytedance|Bytespider|CCBot|DiamondBot|DotBot|EARTHCOM\.info|FurlBot|FyberSpider|GoogleOther|Holmes|htdig|ichiro|ImagesiftBot|IssueCrawler|Jaxified\ Bot|Jyxobot|Larbin|LDSpider|LinkWalker|lmspider|lwp-trivial|magpie-crawler|Mail\.RU_bot|masscan-ng|MLBot|mogimogi|MSRBot|MVAClient|Nusearch\ Spider|Nymesis|Orbbot|polybot|PycURL|Radian6|semrush|SEOChat|Snappy|SurveyBot|TinEye|TurnitinBot|Vortex|webcollage|Xaldon_WebSpider|yacy|Yasaklibot) 1;
	~*(Accoona-AI-Agent|Applebot|Baiduspider|BecomeBot|BeslistBot|Bingbot|CatchBot|coccocbot|cosmos|DataparkSearch|DuckDuckBot|Exabot|Facebook|genieBot|Google-PageRenderer|Googlebot|ia_archiver|LexxeBot|Linguee\ Bot|MJ12bot|MojeekBot|msnbot|NutchCVS|omgilibot|PetalBot|PetalSearch|RogerBot|Scrubby|ShopWiki|sogou\ spider|Speedy\ Spider|StackRambler|Swiftbot|Teoma|TwengaBot|VoilaBot|Websquash|Slurp|YahooSeeker|Yandex|Yeti|YodaoBot|yoogliFetchAgent|ZyBorg) 2;
}

# IPs we want to apply custom rates to. The value mapped to determines the zone we later apply
# Can be either individual IPs or CIDR ranges
geo $known_ips {
	default 0;

	# Nitropack servers
	46.101.77.196 1;
	157.245.30.12 1;
	178.62.71.222 1;
	178.62.81.205 1;

	# Humans
	111.222.333.444 2; # Example human
}

# Now we create variables that are either set to the requesting nodes IP, or an empty string depending on how it maps to the above lists
map "$known_bots:$known_ips" $heavily_restricted_bot {
	default "";
	"1:0" $binary_remote_addr;
}

map "$known_bots:$known_ips" $less_restricted_bot {
	default "";
	"2:0" $binary_remote_addr;
}

map "$known_bots:$known_ips" $nitropack_server {
	default "";
	"0:1" $binary_remote_addr;
}

map "$known_bots:$known_ips" $authorised_human {
	default "";
	"0:2" $binary_remote_addr;
}

map "$known_bots:$known_ips" $regular_ip {
	default "";
	"0:0" $binary_remote_addr;
}

# And turn these variables into rate limit zones, setting a number of requests to allow by this node per second or minute
# 10mb shared memory slots should be able to hold approx 8,000 to 160,000 IPs at any one time. Ref https://nginx.org/en/docs/http/ngx_http_limit_req_module.html
limit_req_zone $heavily_restricted_bot zone=heavily_restricted_bots:10m rate=3r/m;
limit_req_zone $less_restricted_bot zone=less_restricted_bots:10m rate=6r/m;
limit_req_zone $nitropack_server zone=nitropack_servers:10m rate=4r/s;
limit_req_zone $authorised_human zone=authorised_humans:10m rate=60r/s;
limit_req_zone $regular_ip zone=regular_ips:10m rate=30r/s;

# Enables each rate limiting zone and sets their bursts, i.e. the number of requests allowed to be queued that exceed the above rates, which are then processed at the above rates
limit_req zone=heavily_restricted_bots burst=10;
limit_req zone=less_restricted_bots burst=20;
limit_req zone=nitropack_servers burst=100;
limit_req zone=authorised_humans burst=200;
limit_req zone=regular_ips burst=100;

# Requests that exceed the rate and don't fit in the burst queue are dropped
limit_req_status 429;
